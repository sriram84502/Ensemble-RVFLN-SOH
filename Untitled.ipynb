{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3b996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc65507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0005 = pd.read_csv('./preprocessed_data/B0005_preprocessed.csv')\n",
    "df0006 = pd.read_csv('./preprocessed_data/B0005_preprocessed.csv')\n",
    "df0007 = pd.read_csv('./preprocessed_data/B0005_preprocessed.csv')\n",
    "df0018 = pd.read_csv('./preprocessed_data/B0005_preprocessed.csv')\n",
    "df0005e = pd.read_csv('./encrypted_datasets/B0005e.csv')\n",
    "df0006e = pd.read_csv('./encrypted_datasets/B0006e.csv')\n",
    "df0007e = pd.read_csv('./encrypted_datasets/B0007e.csv')\n",
    "df0018e = pd.read_csv('./encrypted_datasets/B0018e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab0d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "soh0005 = []\n",
    "soh0006 = []\n",
    "soh0007 = []\n",
    "soh0018 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c38a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sohc(a,b):\n",
    "    if len(b)==0:\n",
    "        C = a['capacity'][0]\n",
    "        for i in range(len(a)):\n",
    "          b.append([a['capacity'][i] / C])\n",
    "    b = pd.DataFrame(data=b, columns=['soh'])\n",
    "    a['soh'] = pd.DataFrame(data=b, columns=['soh'])\n",
    "    X = a[['cycle', 'voltage_measured', 'current_measured', 'temperature_measured', 'current_load', 'voltage_load', 'time']]\n",
    "    y = a['soh']\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_train0005, X_test0005, y_train0005, y_test0005 = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    return X_train0005, X_test0005, y_train0005, y_test0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac97cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0005, X_test0005, y_train0005, y_test0005 = sohc(df0005,soh0005)\n",
    "X_train0006, X_test0006, y_train0006, y_test0006 = sohc(df0006,soh0006)\n",
    "X_train0007, X_test0007, y_train0007, y_test0007 = sohc(df0007,soh0007)\n",
    "X_train0018, X_test0018, y_train0018, y_test0018 = sohc(df0018,soh0018)\n",
    "X_train0005e, X_test0005e, y_train0005e, y_test0005e = sohc(df0005e,soh0005)\n",
    "X_train0006e, X_test0006e, y_train0006e, y_test0006e = sohc(df0006e,soh0006)\n",
    "X_train0007e, X_test0007e, y_train0007e, y_test0007e = sohc(df0007e,soh0007)\n",
    "X_train0018e, X_test0018e, y_train0018e, y_test0018e = sohc(df0018e,soh0018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459f97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rvfl_model(input_size=7, hidden_sizes=[64,32,16], output_size=1):\n",
    "    inputs = tf.keras.Input(shape=(input_size,))\n",
    "    x = inputs\n",
    "    for i, hidden_size in enumerate(hidden_sizes):\n",
    "        fixed_weights = np.random.uniform(-1, 1, (input_size if i == 0 else hidden_sizes[i-1], hidden_size)).astype(np.float32)\n",
    "        fixed_biases = np.random.uniform(-1, 1, (hidden_size,)).astype(np.float32)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(units=hidden_size,\n",
    "                                  activation='relu',\n",
    "                                  trainable=(i == len(hidden_sizes) - 1),  # Only the last hidden layer is trainable\n",
    "                                  kernel_initializer=tf.constant_initializer(fixed_weights),\n",
    "                                  bias_initializer=tf.constant_initializer(fixed_biases))(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units=output_size, activation='sigmoid', trainable=True)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def train_rvfl_model(model, X_train, y_train, learning_rate=0.01, epochs=100,i=\"B0005\",j=1):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mae')\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"results/model_mae_{i}_model_{j}\",histogram_freq=1)\n",
    "    model.fit(X_train, y_train, epochs=epochs, verbose=1,callbacks=tb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df988e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 10:00:35.679687: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 1s 532us/step - loss: 0.0341\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0059\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0052\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 502us/step - loss: 0.0051\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0050\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0050\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0049\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0049\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0047\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0047\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0047\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0046\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0045\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0045\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0045\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0044\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0044\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0043\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0043\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 487us/step - loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 518us/step - loss: 0.0131\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0078\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0067\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0068\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0062\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0059\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 502us/step - loss: 0.0060\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 519us/step - loss: 0.0059\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0058\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0058\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0055\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0054\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0055\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0052\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 512us/step - loss: 0.0052\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0054\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0053\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0052\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0051\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 501us/step - loss: 0.0295\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0178\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0124\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0099\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0092\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0088\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0086\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0083\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0082\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0080\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0078\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0076\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0074\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0072\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0070\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0070\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0068\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.0066\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0066\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 487us/step - loss: 0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.1373\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0191\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0188\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0186\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0185\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0184\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0183\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0182\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0180\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0173\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0166\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0165\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0163\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0161\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0160\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0159\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0159\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0156\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0147\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.3362\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.1472\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.1436\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.1435\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.1434\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.1433\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.1432\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.1432\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.1431\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.1317\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0928\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0761\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0663\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0608\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0569\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0534\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0505\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0482\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0463\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0228\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0063\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0057\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0055\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0054\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0053\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0052\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0051\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0051\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0050\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0050\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0050\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0049\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0049\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0048\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0048\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0047\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0046\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0047\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 515us/step - loss: 0.0285\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0075\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0067\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0067\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0065\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0060\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 516us/step - loss: 0.0064\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0060\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0060\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0057\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0059\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0056\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0057\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0057\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0055\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0054\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0054\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0056\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 517us/step - loss: 0.0053\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 512us/step - loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0311\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0106\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0093\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0087\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0080\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0077\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0075\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0075\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0073\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0072\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0071\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 498us/step - loss: 0.0072\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0069\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0069\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0069\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0068\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0068\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0067\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0067\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 500us/step - loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.1137\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0530\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0463\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0394\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0359\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0342\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0332\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0326\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0322\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0317\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0310\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0304\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0299\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0295\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0292\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0288\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0281\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0274\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0271\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0717\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0596\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0589\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0541\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0531\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0529\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0528\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0527\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0527\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0527\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0527\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0527\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0527\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0527\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0527\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0527\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0527\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0527\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0527\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0241\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0061\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0056\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0054\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0054\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0052\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0052\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0051\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0050\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0050\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0049\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0048\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0047\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0047\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0046\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0046\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0045\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 487us/step - loss: 0.0045\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0044\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0145\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0067\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0062\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0060\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0058\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0058\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 516us/step - loss: 0.0056\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 519us/step - loss: 0.0056\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0053\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0055\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 502us/step - loss: 0.0053\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0053\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0056\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0052\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 516us/step - loss: 0.0051\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 518us/step - loss: 0.0051\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 521us/step - loss: 0.0050\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0049\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0051\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 502us/step - loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0329\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0108\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0082\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 501us/step - loss: 0.0073\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0070\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0067\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0065\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0064\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0063\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0062\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0061\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0060\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0060\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0059\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0059\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0058\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0057\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0057\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0057\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.1597\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.1597\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.1597\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.1597\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.1597\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.1597\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.1597\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.1597\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.1597\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.1597\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.1597\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.1597\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.1597\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.1597\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.1597\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.1597\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.1597\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.1597\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.1597\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.1597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.1608\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.1114\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0574\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0554\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0532\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0504\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 501us/step - loss: 0.0410\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0257\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0212\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0205\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0202\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.0201\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.0201\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0200\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0200\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0200\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0199\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0199\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0199\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0206\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0063\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0056\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0053\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0052\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0052\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 487us/step - loss: 0.0051\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0051\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0049\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0049\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0048\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0048\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0047\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0047\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0047\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0046\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0045\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0045\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0045\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 517us/step - loss: 0.0113\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 513us/step - loss: 0.0069\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0065\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0063\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0061\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0058\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0058\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0057\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0057\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0053\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0056\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 513us/step - loss: 0.0054\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 519us/step - loss: 0.0055\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 520us/step - loss: 0.0054\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0053\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0051\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0050\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0053\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0051\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 520us/step - loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0734\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0189\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 499us/step - loss: 0.0177\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0170\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0167\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0160\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0158\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0154\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0152\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0150\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0150\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0147\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0146\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0145\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 498us/step - loss: 0.0145\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0141\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0140\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0140\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 500us/step - loss: 0.0140\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 487us/step - loss: 0.0524\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0148\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0114\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0105\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0102\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0099\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0094\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0092\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0090\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0088\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0088\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0087\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0086\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0085\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0085\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0083\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0084\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0083\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0082\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.1084\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 487us/step - loss: 0.0330\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0313\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0302\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0293\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0283\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0269\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0252\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0241\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0236\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0234\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0231\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0230\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0228\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0227\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0226\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0225\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0224\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0224\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0948\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0872\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0872\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0871\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0871\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0871\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0870\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0870\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0870\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0869\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0869\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0869\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0869\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0869\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0868\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.0869\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0868\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 498us/step - loss: 0.0869\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 499us/step - loss: 0.0869\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 502us/step - loss: 0.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 541us/step - loss: 0.0953\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 549us/step - loss: 0.0898\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 532us/step - loss: 0.0889\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 537us/step - loss: 0.0887\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 541us/step - loss: 0.0886\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 537us/step - loss: 0.0883\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 525us/step - loss: 0.0883\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 520us/step - loss: 0.0882\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 532us/step - loss: 0.0881\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 536us/step - loss: 0.0877\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 525us/step - loss: 0.0878\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 525us/step - loss: 0.0876\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 527us/step - loss: 0.0876\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 535us/step - loss: 0.0877\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 524us/step - loss: 0.0876\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 543us/step - loss: 0.0874\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 553us/step - loss: 0.0873\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 527us/step - loss: 0.0873\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 514us/step - loss: 0.0872\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 515us/step - loss: 0.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.1038\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0897\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0886\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0886\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0882\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 525us/step - loss: 0.0881\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0880\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0881\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0879\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0879\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0879\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0876\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0877\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0877\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0876\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0876\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0876\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0877\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0876\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.1590\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.1213\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 466us/step - loss: 0.0934\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0898\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 467us/step - loss: 0.0893\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 467us/step - loss: 0.0889\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0886\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 466us/step - loss: 0.0886\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 468us/step - loss: 0.0882\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 466us/step - loss: 0.0882\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 466us/step - loss: 0.0882\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0879\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0880\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0880\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0878\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0878\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 467us/step - loss: 0.0877\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 468us/step - loss: 0.0878\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 467us/step - loss: 0.0877\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 466us/step - loss: 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.1791\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0906\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0882\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0875\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0872\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0870\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0869\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0869\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0869\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 489us/step - loss: 0.0869\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0869\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0869\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0869\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0869\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0869\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0869\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0869\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0869\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0869\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0928\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0874\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0875\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0874\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0873\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0872\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0873\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0872\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0872\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0871\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0871\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0871\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0870\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.0871\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0871\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0870\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0870\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0869\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0870\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 521us/step - loss: 0.0925\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 520us/step - loss: 0.0896\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 516us/step - loss: 0.0890\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 517us/step - loss: 0.0886\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0884\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0884\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0883\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 558us/step - loss: 0.0881\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 541us/step - loss: 0.0881\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 527us/step - loss: 0.0879\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 536us/step - loss: 0.0877\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0877\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 543us/step - loss: 0.0877\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 530us/step - loss: 0.0877\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 523us/step - loss: 0.0876\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 528us/step - loss: 0.0875\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 532us/step - loss: 0.0875\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 538us/step - loss: 0.0874\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 541us/step - loss: 0.0874\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 526us/step - loss: 0.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 523us/step - loss: 0.1362\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 517us/step - loss: 0.0908\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0889\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0882\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 502us/step - loss: 0.0880\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0879\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0878\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0875\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0876\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 503us/step - loss: 0.0876\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0874\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0875\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0875\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0874\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0874\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0873\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0874\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0872\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0873\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 518us/step - loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0966\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0895\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0888\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0882\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0881\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0879\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0877\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0876\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0875\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0874\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0873\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0872\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0872\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0871\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0871\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0871\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0871\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0870\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0870\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.1782\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0913\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0875\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0871\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0871\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.0871\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0871\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0871\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0871\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0870\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0870\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0870\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0870\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0870\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0871\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0870\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0870\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 480us/step - loss: 0.0870\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0870\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0951\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0874\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.0873\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 474us/step - loss: 0.0872\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0872\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0873\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0872\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0871\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 491us/step - loss: 0.0871\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0872\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0870\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0871\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0870\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0871\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0870\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0870\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 477us/step - loss: 0.0869\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0870\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0870\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0926\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0893\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0891\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0886\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 513us/step - loss: 0.0884\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0882\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0881\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0879\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0876\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0876\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 521us/step - loss: 0.0876\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 511us/step - loss: 0.0876\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0876\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0875\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0874\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0873\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 516us/step - loss: 0.0874\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 517us/step - loss: 0.0872\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0872\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 509us/step - loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 500us/step - loss: 0.1161\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0930\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0908\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 506us/step - loss: 0.0898\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0890\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0887\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 505us/step - loss: 0.0885\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 497us/step - loss: 0.0882\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0882\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0883\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0878\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 501us/step - loss: 0.0879\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0878\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 508us/step - loss: 0.0880\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 510us/step - loss: 0.0878\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 507us/step - loss: 0.0876\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 504us/step - loss: 0.0876\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0877\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 496us/step - loss: 0.0876\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 473us/step - loss: 0.1325\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0905\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0883\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 479us/step - loss: 0.0880\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0879\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 481us/step - loss: 0.0877\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 475us/step - loss: 0.0875\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0878\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 468us/step - loss: 0.0877\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0875\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 472us/step - loss: 0.0877\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 471us/step - loss: 0.0874\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0875\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0875\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 468us/step - loss: 0.0876\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 476us/step - loss: 0.0876\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 478us/step - loss: 0.0875\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0875\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 470us/step - loss: 0.0876\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 469us/step - loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 1s 488us/step - loss: 0.2394\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 1s 492us/step - loss: 0.0912\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0869\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0868\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0868\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0868\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0868\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 1s 490us/step - loss: 0.0868\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 1s 483us/step - loss: 0.0868\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0868\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 1s 484us/step - loss: 0.0868\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 1s 485us/step - loss: 0.0868\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0868\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0868\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0868\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 1s 493us/step - loss: 0.0868\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 1s 494us/step - loss: 0.0868\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 1s 495us/step - loss: 0.0868\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 1s 486us/step - loss: 0.0868\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 1s 482us/step - loss: 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "872/872 [==============================] - 1s 488us/step - loss: 0.0828\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0661\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 0s 482us/step - loss: 0.0660\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 0s 485us/step - loss: 0.0661\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 0s 482us/step - loss: 0.0660\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0659\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 0s 480us/step - loss: 0.0660\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 0s 480us/step - loss: 0.0659\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 0s 490us/step - loss: 0.0659\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 0s 492us/step - loss: 0.0659\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 0s 494us/step - loss: 0.0659\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 0s 494us/step - loss: 0.0658\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 0s 490us/step - loss: 0.0658\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 0s 482us/step - loss: 0.0659\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 0s 483us/step - loss: 0.0658\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0658\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0658\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0657\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 0s 489us/step - loss: 0.0657\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "872/872 [==============================] - 1s 516us/step - loss: 0.0774\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 0s 510us/step - loss: 0.0674\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 0s 513us/step - loss: 0.0671\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 0s 512us/step - loss: 0.0668\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 0s 510us/step - loss: 0.0666\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 0s 510us/step - loss: 0.0664\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 0s 511us/step - loss: 0.0662\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 0s 510us/step - loss: 0.0663\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 0s 509us/step - loss: 0.0664\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 0s 520us/step - loss: 0.0661\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 0s 517us/step - loss: 0.0660\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 0s 510us/step - loss: 0.0662\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 0s 509us/step - loss: 0.0660\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 0s 511us/step - loss: 0.0660\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 0s 507us/step - loss: 0.0660\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 0s 509us/step - loss: 0.0659\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 0s 522us/step - loss: 0.0658\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 0s 521us/step - loss: 0.0658\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 0s 519us/step - loss: 0.0659\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 0s 519us/step - loss: 0.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "872/872 [==============================] - 1s 508us/step - loss: 0.0886\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0700\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0682\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 0s 502us/step - loss: 0.0674\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 0s 504us/step - loss: 0.0671\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 0s 515us/step - loss: 0.0669\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 0s 504us/step - loss: 0.0668\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 0s 508us/step - loss: 0.0666\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0665\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0664\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 0s 502us/step - loss: 0.0664\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 0s 511us/step - loss: 0.0664\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 0s 515us/step - loss: 0.0662\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 0s 509us/step - loss: 0.0663\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0662\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0662\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0660\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 0s 503us/step - loss: 0.0661\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 0s 504us/step - loss: 0.0661\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 0s 516us/step - loss: 0.0661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "872/872 [==============================] - 1s 489us/step - loss: 0.1033\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0700\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 0s 483us/step - loss: 0.0687\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 0s 482us/step - loss: 0.0680\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 0s 482us/step - loss: 0.0676\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0673\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 0s 491us/step - loss: 0.0670\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 0s 481us/step - loss: 0.0669\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0667\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 0s 483us/step - loss: 0.0666\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 0s 488us/step - loss: 0.0665\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 0s 483us/step - loss: 0.0664\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 0s 490us/step - loss: 0.0663\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 0s 493us/step - loss: 0.0663\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0663\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 0s 482us/step - loss: 0.0662\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0662\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 0s 483us/step - loss: 0.0662\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 0s 483us/step - loss: 0.0661\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "872/872 [==============================] - 1s 496us/step - loss: 0.0851\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 0s 486us/step - loss: 0.0700\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0685\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0676\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 0s 485us/step - loss: 0.0671\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 0s 497us/step - loss: 0.0668\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 0s 499us/step - loss: 0.0665\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 0s 498us/step - loss: 0.0663\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 0s 499us/step - loss: 0.0662\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 0s 502us/step - loss: 0.0661\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 0s 498us/step - loss: 0.0661\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 0s 499us/step - loss: 0.0660\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 0s 498us/step - loss: 0.0659\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 0s 501us/step - loss: 0.0659\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 0s 502us/step - loss: 0.0659\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 0s 497us/step - loss: 0.0659\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 0s 498us/step - loss: 0.0659\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 0s 484us/step - loss: 0.0659\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 0s 485us/step - loss: 0.0658\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 0s 488us/step - loss: 0.0658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 5s 3ms/step - loss: 0.0213\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0101\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 4s 4ms/step - loss: 0.0097\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 4s 4ms/step - loss: 0.0097\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 4s 4ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 4s 4ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0109\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 6s 4ms/step - loss: 0.0098\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 3s 3ms/step - loss: 0.0097\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 3s 3ms/step - loss: 0.0097\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 3s 3ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 3s 3ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 7s 5ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 5s 3ms/step - loss: 0.0248\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0105\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0102\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0100\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0100\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0099\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0099\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0098\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 9s 4ms/step - loss: 0.0316\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 6s 4ms/step - loss: 0.0112\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0105\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0101\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0100\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0099\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0099\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0098\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 13s 11ms/step - loss: 0.0098\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 16s 13ms/step - loss: 0.0098\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 12s 9ms/step - loss: 0.0098\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 8s 7ms/step - loss: 0.0098\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 14s 11ms/step - loss: 0.0098\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 21s 14ms/step - loss: 0.0158\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 16s 13ms/step - loss: 0.0101\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 14s 11ms/step - loss: 0.0099\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0098\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0098\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0098\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0097\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0097\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 11s 8ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 13s 7ms/step - loss: 0.0487\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 7s 5ms/step - loss: 0.0112\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0103\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0100\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0098\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0096\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0096\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 18s 12ms/step - loss: 0.0146\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0097\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 8s 7ms/step - loss: 0.0098\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 8s 7ms/step - loss: 0.0097\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0097\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 12s 10ms/step - loss: 0.0097\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0097\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0097\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 13s 10ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 23s 18ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 19s 15ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 12s 10ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 19s 10ms/step - loss: 0.0213\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 9s 8ms/step - loss: 0.0102\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0101\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0100\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0100\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0099\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 13s 10ms/step - loss: 0.0099\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 8s 7ms/step - loss: 0.0098\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0098\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 7s 5ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 7s 3ms/step - loss: 0.0177\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0104\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 6s 5ms/step - loss: 0.0100\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0099\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0099\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0098\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 6s 4ms/step - loss: 0.0097\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 4s 3ms/step - loss: 0.0097\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 4s 4ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 4s 4ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0097\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 5s 4ms/step - loss: 0.0097\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 16s 12ms/step - loss: 0.0673\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0129\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0104\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 14s 11ms/step - loss: 0.0100\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0099\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0099\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 12s 10ms/step - loss: 0.0098\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 17s 14ms/step - loss: 0.0098\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0097\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 13s 10ms/step - loss: 0.0098\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 1/20\n",
      "1258/1258 [==============================] - 15s 8ms/step - loss: 0.0154\n",
      "Epoch 2/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0100\n",
      "Epoch 3/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0099\n",
      "Epoch 4/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0099\n",
      "Epoch 5/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 6/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 7/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0098\n",
      "Epoch 8/20\n",
      "1258/1258 [==============================] - 9s 7ms/step - loss: 0.0098\n",
      "Epoch 9/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0098\n",
      "Epoch 10/20\n",
      "1258/1258 [==============================] - 8s 6ms/step - loss: 0.0098\n",
      "Epoch 11/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0098\n",
      "Epoch 12/20\n",
      "1258/1258 [==============================] - 7s 6ms/step - loss: 0.0098\n",
      "Epoch 13/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 14/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0098\n",
      "Epoch 15/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0099\n",
      "Epoch 16/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0098\n",
      "Epoch 17/20\n",
      "1258/1258 [==============================] - 11s 9ms/step - loss: 0.0098\n",
      "Epoch 18/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 19/20\n",
      "1258/1258 [==============================] - 12s 9ms/step - loss: 0.0098\n",
      "Epoch 20/20\n",
      "1258/1258 [==============================] - 10s 8ms/step - loss: 0.0098\n",
      "Epoch 1/20\n",
      "872/872 [==============================] - 11s 8ms/step - loss: 0.0075\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 8s 10ms/step - loss: 0.0055\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 10s 11ms/step - loss: 0.0055\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0055\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0054\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0054\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0054\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 10s 11ms/step - loss: 0.0054\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 4s 5ms/step - loss: 0.0054\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 4s 5ms/step - loss: 0.0054\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0054\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0054\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 10s 12ms/step - loss: 0.0054\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0054\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0054\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 1/20\n",
      "872/872 [==============================] - 12s 10ms/step - loss: 0.0065\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0057\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0057\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 10s 12ms/step - loss: 0.0056\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 12s 13ms/step - loss: 0.0057\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 11s 12ms/step - loss: 0.0056\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0056\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0056\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 5s 6ms/step - loss: 0.0056\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 4s 5ms/step - loss: 0.0056\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 5s 5ms/step - loss: 0.0056\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0056\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0056\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0055\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0056\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0055\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0056\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 9s 11ms/step - loss: 0.0055\n",
      "Epoch 1/20\n",
      "872/872 [==============================] - 10s 8ms/step - loss: 0.0088\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0062\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0058\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0057\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 12s 14ms/step - loss: 0.0056\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0055\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 10s 11ms/step - loss: 0.0055\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 8s 10ms/step - loss: 0.0055\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0055\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 7s 9ms/step - loss: 0.0055\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 8s 10ms/step - loss: 0.0055\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0055\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0055\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0055\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0054\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0054\n",
      "Epoch 1/20\n",
      "872/872 [==============================] - 11s 8ms/step - loss: 0.0075\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0064\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0060\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0058\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 7s 7ms/step - loss: 0.0057\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0056\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0055\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 7s 9ms/step - loss: 0.0055\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0055\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 11s 12ms/step - loss: 0.0054\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 8s 10ms/step - loss: 0.0054\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0054\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0054\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0054\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0054\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0054\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 10s 11ms/step - loss: 0.0054\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 9s 10ms/step - loss: 0.0054\n",
      "Epoch 1/20\n",
      "872/872 [==============================] - 14s 11ms/step - loss: 0.0210\n",
      "Epoch 2/20\n",
      "872/872 [==============================] - 10s 12ms/step - loss: 0.0167\n",
      "Epoch 3/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0146\n",
      "Epoch 4/20\n",
      "872/872 [==============================] - 8s 9ms/step - loss: 0.0061\n",
      "Epoch 5/20\n",
      "872/872 [==============================] - 9s 11ms/step - loss: 0.0057\n",
      "Epoch 6/20\n",
      "872/872 [==============================] - 11s 12ms/step - loss: 0.0056\n",
      "Epoch 7/20\n",
      "872/872 [==============================] - 10s 12ms/step - loss: 0.0055\n",
      "Epoch 8/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 9/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0054\n",
      "Epoch 10/20\n",
      "872/872 [==============================] - 6s 7ms/step - loss: 0.0055\n",
      "Epoch 11/20\n",
      "872/872 [==============================] - 8s 10ms/step - loss: 0.0054\n",
      "Epoch 12/20\n",
      "872/872 [==============================] - 10s 11ms/step - loss: 0.0054\n",
      "Epoch 13/20\n",
      "872/872 [==============================] - 10s 11ms/step - loss: 0.0054\n",
      "Epoch 14/20\n",
      "872/872 [==============================] - 9s 11ms/step - loss: 0.0054\n",
      "Epoch 15/20\n",
      "872/872 [==============================] - 15s 17ms/step - loss: 0.0054\n",
      "Epoch 16/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0054\n",
      "Epoch 17/20\n",
      "872/872 [==============================] - 10s 12ms/step - loss: 0.0054\n",
      "Epoch 18/20\n",
      "872/872 [==============================] - 10s 12ms/step - loss: 0.0054\n",
      "Epoch 19/20\n",
      "872/872 [==============================] - 12s 14ms/step - loss: 0.0054\n",
      "Epoch 20/20\n",
      "872/872 [==============================] - 7s 8ms/step - loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "def train_ensemble_models(X_train, y_train, n_models=5, input_size=7, hidden_sizes=[64,32,16], output_size=1, learning_rate=0.001, epochs=20,i=\"\"):\n",
    "    models = []\n",
    "    j=0\n",
    "    for hidden_size in hidden_sizes:\n",
    "        j=j+1\n",
    "        model = create_rvfl_model(input_size, hidden_size, output_size)\n",
    "        train_rvfl_model(model, X_train, y_train, learning_rate, epochs,i,j)\n",
    "        pickle.dump(i,open(f\"models/{i}_{j}.pkl\",'wb'))\n",
    "        models.append(model)\n",
    "    pickle.dump(models,open(f\"models/{i}.pkl\",'wb'))\n",
    "    return models\n",
    "\n",
    "def ensemble_predict(models, X_test):\n",
    "    predictions = [model.predict(X_test) for model in models]\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    return ensemble_predictions\n",
    "\n",
    "n_models = 5\n",
    "input_size = 7\n",
    "hidden_sizes = [[100],[100,50],[64,32,16],[64,32,16,8],[64,32,16,8,4]]\n",
    "output_size = 1\n",
    "\n",
    "models0005 = train_ensemble_models(X_train0005, y_train0005, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0005\")\n",
    "models0006 = train_ensemble_models(X_train0006, y_train0006, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0006\")\n",
    "models0007 = train_ensemble_models(X_train0007, y_train0007, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0007\")\n",
    "models0018 = train_ensemble_models(X_train0018, y_train0018, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0018\")\n",
    "models0005e = train_ensemble_models(X_train0005e, y_train0005e, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0005e\")\n",
    "models0006e = train_ensemble_models(X_train0006e, y_train0006e, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0006e\")\n",
    "models0007e = train_ensemble_models(X_train0007e, y_train0007e, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0007e\")\n",
    "models0018e = train_ensemble_models(X_train0018e, y_train0018e, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20,i=\"B0018e\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342131d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, X_test):\n",
    "    predictions = [model.predict(X_test) for model in models]\n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    return ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49d8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 0s 348us/step\n",
      "315/315 [==============================] - 0s 342us/step\n",
      "315/315 [==============================] - 0s 355us/step\n",
      "315/315 [==============================] - 0s 339us/step\n",
      "315/315 [==============================] - 0s 340us/step\n",
      "315/315 [==============================] - 0s 298us/step\n",
      "315/315 [==============================] - 0s 325us/step\n",
      "315/315 [==============================] - 0s 321us/step\n",
      "315/315 [==============================] - 0s 333us/step\n",
      "315/315 [==============================] - 0s 336us/step\n",
      "315/315 [==============================] - 0s 299us/step\n",
      "315/315 [==============================] - 0s 329us/step\n",
      "315/315 [==============================] - 0s 327us/step\n",
      "315/315 [==============================] - 0s 333us/step\n",
      "315/315 [==============================] - 0s 338us/step\n",
      "315/315 [==============================] - 0s 300us/step\n",
      "315/315 [==============================] - 0s 323us/step\n",
      "315/315 [==============================] - 0s 328us/step\n",
      "315/315 [==============================] - 0s 331us/step\n",
      "315/315 [==============================] - 0s 335us/step\n",
      "315/315 [==============================] - 0s 299us/step\n",
      "315/315 [==============================] - 0s 326us/step\n",
      "315/315 [==============================] - 0s 328us/step\n",
      "315/315 [==============================] - 0s 327us/step\n",
      "315/315 [==============================] - 0s 338us/step\n",
      "315/315 [==============================] - 0s 300us/step\n",
      "315/315 [==============================] - 0s 324us/step\n",
      "315/315 [==============================] - 0s 329us/step\n",
      "315/315 [==============================] - 0s 334us/step\n",
      "315/315 [==============================] - 0s 341us/step\n",
      "315/315 [==============================] - 0s 299us/step\n",
      "315/315 [==============================] - 0s 325us/step\n",
      "315/315 [==============================] - 0s 327us/step\n",
      "315/315 [==============================] - 0s 333us/step\n",
      "315/315 [==============================] - 0s 338us/step\n",
      "218/218 [==============================] - 0s 319us/step\n",
      "218/218 [==============================] - 0s 334us/step\n",
      "218/218 [==============================] - 0s 333us/step\n",
      "218/218 [==============================] - 0s 344us/step\n",
      "218/218 [==============================] - 0s 348us/step\n"
     ]
    }
   ],
   "source": [
    "B0005p = ensemble_predict(models0005,X_test0005)\n",
    "B0006p = ensemble_predict(models0006,X_test0006)\n",
    "B0007p = ensemble_predict(models0007,X_test0007)\n",
    "B0018p = ensemble_predict(models0018,X_test0018)\n",
    "B0005ep = ensemble_predict(models0005e,X_test0005e)\n",
    "B0006ep = ensemble_predict(models0006e,X_test0006e)\n",
    "B0007ep = ensemble_predict(models0007e,X_test0007e)\n",
    "B0018ep = ensemble_predict(models0018e,X_test0018e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c50e75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = pd.read_csv('./encrypted_datasets/B0005e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9221b2eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m\u001b[43msoh\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soh' is not defined"
     ]
    }
   ],
   "source": [
    "dfe['soh'] = pd.DataFrame(data=soh, columns=['soh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ebcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = df[['cycle', 'voltage_measured', 'current_measured', 'temperature_measured', 'current_load', 'voltage_load', 'time']]\n",
    "ye = df['soh']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scalede = scaler.fit_transform(X)\n",
    "\n",
    "X_traine, X_teste, y_traine, y_teste = train_test_split(X_scalede, ye, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fac832",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelse = train_ensemble_models(X_traine, y_traine, n_models=n_models, input_size=input_size, hidden_sizes=hidden_sizes, output_size=output_size, learning_rate=0.001, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predse = ensemble_predict(models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_predse[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adda46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_teste[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55315744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "axs[0, 0].scatter(B0005p, y_test0005, color='blue', marker='o')\n",
    "axs[0, 0].set_xlabel('Predicted SoH')\n",
    "axs[0, 0].set_ylabel('Actual SoH')\n",
    "axs[0, 0].set_title('Actual vs. Predicted SoH in B0005')\n",
    "\n",
    "# Plot the second graph (top-right)\n",
    "axs[0, 1].scatter(B0006p, y_test0006, color='red', marker='x')\n",
    "axs[0, 1].set_xlabel('Predicted SoH')\n",
    "axs[0, 1].set_ylabel('Actual SoH')\n",
    "axs[0, 1].set_title('Actual vs. Predicted SoH in B0006')\n",
    "\n",
    "# Plot the third graph (bottom-left)\n",
    "axs[1, 0].scatter(B0007p, y_test0007, color='green', marker='s')\n",
    "axs[1, 0].set_xlabel('Predicted SoH')\n",
    "axs[1, 0].set_ylabel('Actual SoH')\n",
    "axs[1, 0].set_title('Actual vs. Predicted SoH in B0007')\n",
    "\n",
    "# Plot the fourth graph (bottom-right)\n",
    "axs[1, 1].scatter(B0018p, y_test0018, color='purple', marker='^')\n",
    "axs[1, 1].set_xlabel('Predicted SoH')\n",
    "axs[1, 1].set_ylabel('Actual SoH')\n",
    "axs[1, 1].set_title('Actual vs. Predicted SoH in B0018')\n",
    "\n",
    "# Adjust layout to prevent overlapping titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0851b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c4c178-81ff-476d-9dee-bba94c49e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f20e58-6162-4554-8649-c1114baf9dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/Myproject/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/anaconda3/envs/Myproject/lib/python3.10/site-packages (from scipy) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f526ea50-7aeb-4a12-85e5-7eead74b5719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic: 2.5, p-value: 0.625\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "data1 = [0.098,0.114,0.080,0.084] \n",
    "data2 = [0.097, 0.097, 0.097, 0.054]\n",
    "\n",
    "stat, p = wilcoxon(data1, data2)\n",
    "print(f'Statistic: {stat}, p-value: {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2789f2b1-10b7-4a3b-ba0c-f13145d0be28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic: 3.75, p-value: 0.28975578119338274\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Data organized by conditions (columns) for each subject (rows)\n",
    "data = [\n",
    "    [0.098, 0.097],\n",
    "    [0.114, 0.097],\n",
    "    [0.080, 0.097],\n",
    "    [0.084, 0.054]\n",
    "]\n",
    "\n",
    "# Perform the Friedman test\n",
    "stat, p = friedmanchisquare(*data)\n",
    "print(f'Statistic: {stat}, p-value: {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7418f0-64b1-4294-aaa5-92f82979d393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5, 0.625, 3.75, 0.28975578119338274)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "\n",
    "# Data for Wilcoxon signed-rank test\n",
    "data1 = [0.098, 0.114, 0.080, 0.084] \n",
    "data2 = [0.097, 0.097, 0.097, 0.054]\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "stat_wilcoxon, p_wilcoxon = wilcoxon(data1, data2)\n",
    "stat_wilcoxon, p_wilcoxon, stat_friedman, p_friedman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f0535c-98b0-4191-962c-dee235ea8218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5, 0.625, 3.75, 0.28975578119338274)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Data for Friedman test\n",
    "data_friedman = [\n",
    "    [0.098, 0.097],\n",
    "    [0.114, 0.097],\n",
    "    [0.080, 0.097],\n",
    "    [0.084, 0.054]\n",
    "]\n",
    "\n",
    "# Perform Friedman test\n",
    "stat_friedman, p_friedman = friedmanchisquare(*data_friedman)\n",
    "\n",
    "stat_wilcoxon, p_wilcoxon, stat_friedman, p_friedman"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
